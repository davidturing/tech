#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é¡¹ç›®äºŒï¼šæœºå™¨å­¦ä¹ é¢„æµ‹ç®¡é“ - æˆ¿ä»·é¢„æµ‹ç¤ºä¾‹
å­¦ç”Ÿç¤ºä¾‹ä»£ç 
ä½œè€…: å¼ æ˜ (985é«˜æ ¡å·¥ç§‘å¤§äºŒå­¦ç”Ÿ)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
import joblib
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")

def generate_housing_data():
    """ç”Ÿæˆæ¨¡æ‹Ÿæˆ¿ä»·æ•°æ®ï¼ˆå®é™…é¡¹ç›®ä¸­ä¼šä»CSVæ–‡ä»¶åŠ è½½ï¼‰"""
    np.random.seed(42)
    n_samples = 1000
    
    # ç‰¹å¾ç”Ÿæˆ
    data = {
        'area': np.random.normal(100, 30, n_samples),  # é¢ç§¯ (å¹³æ–¹ç±³)
        'bedrooms': np.random.randint(1, 6, n_samples),  # å§å®¤æ•°é‡
        'bathrooms': np.random.randint(1, 4, n_samples),  # æµ´å®¤æ•°é‡
        'age': np.random.randint(0, 50, n_samples),  # æˆ¿é¾„
        'location_score': np.random.uniform(1, 10, n_samples),  # ä½ç½®è¯„åˆ†
        'school_district': np.random.choice(['A', 'B', 'C'], n_samples, p=[0.3, 0.4, 0.3]),  # å­¦åŒº
        'has_garden': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),  # æ˜¯å¦æœ‰èŠ±å›­
        'has_parking': np.random.choice([0, 1], n_samples, p=[0.6, 0.4]),  # æ˜¯å¦æœ‰åœè½¦ä½
    }
    
    # ç›®æ ‡å˜é‡ç”Ÿæˆï¼ˆåŸºäºç‰¹å¾çš„çº¿æ€§ç»„åˆ + å™ªå£°ï¼‰
    price = (
        data['area'] * 1000 +
        data['bedrooms'] * 20000 +
        data['bathrooms'] * 15000 +
        data['location_score'] * 30000 +
        (50 - data['age']) * 1000 +
        np.where(data['school_district'] == 'A', 50000, 
                np.where(data['school_district'] == 'B', 30000, 10000)) +
        data['has_garden'] * 20000 +
        data['has_parking'] * 15000 +
        np.random.normal(0, 20000, n_samples)  # å™ªå£°
    )
    
    data['price'] = np.maximum(price, 100000)  # ç¡®ä¿ä»·æ ¼ä¸ºæ­£
    
    df = pd.DataFrame(data)
    
    # æ·»åŠ ä¸€äº›ç¼ºå¤±å€¼ç”¨äºæ¼”ç¤ºå¤„ç†è¿‡ç¨‹
    missing_indices = np.random.choice(df.index, size=int(0.05 * len(df)), replace=False)
    df.loc[missing_indices, 'area'] = np.nan
    
    return df

def explore_data(df):
    """æ•°æ®æ¢ç´¢ä¸å¯è§†åŒ–"""
    print("=== æ•°æ®æ¢ç´¢ ===")
    
    print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"\næ•°æ®åŸºæœ¬ä¿¡æ¯:")
    print(df.info())
    print(f"\næ•°å€¼åˆ—ç»Ÿè®¡æ‘˜è¦:")
    print(df.describe())
    
    # å¯è§†åŒ–ç›®æ ‡å˜é‡åˆ†å¸ƒ
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.hist(df['price'], bins=50, alpha=0.7, color='skyblue')
    plt.title('æˆ¿ä»·åˆ†å¸ƒ')
    plt.xlabel('ä»·æ ¼ (å…ƒ)')
    plt.ylabel('é¢‘æ¬¡')
    
    plt.subplot(1, 2, 2)
    plt.boxplot(df['price'])
    plt.title('æˆ¿ä»·ç®±çº¿å›¾')
    plt.ylabel('ä»·æ ¼ (å…ƒ)')
    
    plt.tight_layout()
    plt.savefig('price_distribution.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    correlation_matrix = df[numeric_cols].corr()
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
    plt.title('ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾')
    plt.savefig('feature_correlation.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return df

def prepare_features(df):
    """ç‰¹å¾å·¥ç¨‹"""
    print("\n=== ç‰¹å¾å·¥ç¨‹ ===")
    
    # åˆ›å»ºæ–°ç‰¹å¾
    df['price_per_sqm'] = df['price'] / df['area']  # æ¯å¹³æ–¹ç±³ä»·æ ¼
    df['total_rooms'] = df['bedrooms'] + df['bathrooms']  # æ€»æˆ¿é—´æ•°
    df['is_new'] = (df['age'] <= 5).astype(int)  # æ˜¯å¦ä¸ºæ–°æˆ¿
    
    # å¤„ç†åˆ†ç±»å˜é‡
    le = LabelEncoder()
    df['school_district_encoded'] = le.fit_transform(df['school_district'])
    
    print(f"æ–°å¢ç‰¹å¾: price_per_sqm, total_rooms, is_new, school_district_encoded")
    
    return df, le

def build_models(X_train, X_test, y_train, y_test):
    """æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°"""
    print("\n=== æ¨¡å‹è®­ç»ƒä¸è¯„ä¼° ===")
    
    # å®šä¹‰æ¨¡å‹
    models = {
        'Linear Regression': LinearRegression(),
        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)
    }
    
    results = {}
    
    for name, model in models.items():
        print(f"\nè®­ç»ƒ {name}...")
        
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred = model.predict(X_test)
        
        # è¯„ä¼°æŒ‡æ ‡
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        results[name] = {
            'model': model,
            'mse': mse,
            'rmse': rmse,
            'mae': mae,
            'r2': r2
        }
        
        print(f"{name} æ€§èƒ½:")
        print(f"  RMSE: {rmse:.2f}")
        print(f"  MAE: {mae:.2f}")
        print(f"  RÂ²: {r2:.4f}")
    
    # é€‰æ‹©æœ€ä½³æ¨¡å‹
    best_model_name = max(results.keys(), key=lambda k: results[k]['r2'])
    best_model = results[best_model_name]['model']
    
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name} (RÂ² = {results[best_model_name]['r2']:.4f})")
    
    return results, best_model, best_model_name

def hyperparameter_tuning(X_train, y_train):
    """è¶…å‚æ•°è°ƒä¼˜"""
    print("\n=== è¶…å‚æ•°è°ƒä¼˜ ===")
    
    # Random Forest è¶…å‚æ•°è°ƒä¼˜
    rf_params = {
        'n_estimators': [50, 100, 200],
        'max_depth': [10, 20, None],
        'min_samples_split': [2, 5, 10]
    }
    
    rf = RandomForestRegressor(random_state=42)
    grid_search = GridSearchCV(rf, rf_params, cv=5, scoring='r2', n_jobs=-1)
    grid_search.fit(X_train, y_train)
    
    print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
    print(f"æœ€ä½³äº¤å‰éªŒè¯åˆ†æ•°: {grid_search.best_score_:.4f}")
    
    return grid_search.best_estimator_

def create_prediction_pipeline(best_model, feature_names):
    """åˆ›å»ºé¢„æµ‹ç®¡é“"""
    print("\n=== åˆ›å»ºé¢„æµ‹ç®¡é“ ===")
    
    # åˆ›å»ºå®Œæ•´çš„ç®¡é“ï¼ˆåŒ…æ‹¬é¢„å¤„ç†å’Œæ¨¡å‹ï¼‰
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('model', best_model)
    ])
    
    # ä¿å­˜ç®¡é“
    joblib.dump(pipeline, 'house_price_prediction_pipeline.pkl')
    print("âœ… é¢„æµ‹ç®¡é“å·²ä¿å­˜ä¸º: house_price_prediction_pipeline.pkl")
    
    # åˆ›å»ºä½¿ç”¨ç¤ºä¾‹
    sample_input = pd.DataFrame({
        'area': [120],
        'bedrooms': [3],
        'bathrooms': [2],
        'age': [10],
        'location_score': [8.5],
        'school_district_encoded': [0],  # Aç±»å­¦åŒº
        'has_garden': [1],
        'has_parking': [1],
        'price_per_sqm': [8000],
        'total_rooms': [5],
        'is_new': [0]
    })
    
    # åŠ è½½ç®¡é“å¹¶é¢„æµ‹
    loaded_pipeline = joblib.load('house_price_prediction_pipeline.pkl')
    prediction = loaded_pipeline.predict(sample_input)
    
    print(f"\nç¤ºä¾‹é¢„æµ‹:")
    print(f"è¾“å…¥ç‰¹å¾: {sample_input.iloc[0].to_dict()}")
    print(f"é¢„æµ‹æˆ¿ä»·: Â¥{prediction[0]:,.2f}")
    
    return pipeline

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ‰§è¡Œé¡¹ç›®äºŒï¼šæœºå™¨å­¦ä¹ é¢„æµ‹ç®¡é“ - æˆ¿ä»·é¢„æµ‹")
    
    # 1. æ•°æ®ç”Ÿæˆä¸æ¢ç´¢
    df = generate_housing_data()
    df = explore_data(df)
    
    # 2. ç‰¹å¾å·¥ç¨‹
    df, label_encoder = prepare_features(df)
    
    # 3. å‡†å¤‡è®­ç»ƒæ•°æ®
    feature_cols = ['area', 'bedrooms', 'bathrooms', 'age', 'location_score', 
                   'school_district_encoded', 'has_garden', 'has_parking',
                   'price_per_sqm', 'total_rooms', 'is_new']
    X = df[feature_cols]
    y = df['price']
    
    # å¤„ç†ç¼ºå¤±å€¼
    imputer = SimpleImputer(strategy='median')
    X = pd.DataFrame(imputer.fit_transform(X), columns=feature_cols)
    
    # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # æ ‡å‡†åŒ–ç‰¹å¾
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols)
    X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_cols)
    
    # 4. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°
    results, best_model, best_model_name = build_models(X_train_scaled, X_test_scaled, y_train, y_test)
    
    # 5. è¶…å‚æ•°è°ƒä¼˜
    best_model_tuned = hyperparameter_tuning(X_train_scaled, y_train)
    
    # 6. åˆ›å»ºé¢„æµ‹ç®¡é“
    pipeline = create_prediction_pipeline(best_model_tuned, feature_cols)
    
    print("\nğŸ‰ é¡¹ç›®äºŒæ‰§è¡Œå®Œæˆï¼æ‰€æœ‰è¾“å‡ºæ–‡ä»¶å·²ä¿å­˜åˆ°å½“å‰ç›®å½•ã€‚")
    print("ğŸ“‹ ç”Ÿæˆçš„æ–‡ä»¶åŒ…æ‹¬:")
    print("   - price_distribution.png")
    print("   - feature_correlation.png") 
    print("   - house_price_prediction_pipeline.pkl")

if __name__ == "__main__":
    main()